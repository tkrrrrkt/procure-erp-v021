# ================================================
# ProcureERP Performance & Load Testing Workflow
# ä¼æ¥­ç´šãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ»è² è·ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–
# ================================================

name: âš¡ Performance & Load Testing

on:
  schedule:
    # æ¯Žé€±æ—¥æ›œæ—¥åˆå‰3æ™‚ã«å®Ÿè¡Œ (JST)
    - cron: '0 18 * * 0'
  push:
    branches: [ main ]
    paths:
      - 'backend/**'
      - 'frontend/**'
      - 'docker/**'
      - 'performance-tests/**'
  workflow_dispatch:
    inputs:
      test_duration:
        description: 'Test Duration (minutes)'
        required: true
        default: '5'
        type: string
      virtual_users:
        description: 'Number of Virtual Users'
        required: true
        default: '50'
        type: string
      target_environment:
        description: 'Target Environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production

env:
  NODE_VERSION: '20.x'
  TEST_DURATION: ${{ github.event.inputs.test_duration || '5' }}
  VIRTUAL_USERS: ${{ github.event.inputs.virtual_users || '50' }}
  TARGET_ENV: ${{ github.event.inputs.target_environment || 'staging' }}

jobs:
  # ================================================
  # Setup Test Environment
  # ================================================
  setup-environment:
    name: ðŸ—ï¸ Setup Test Environment
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    outputs:
      test-url: ${{ steps.get-url.outputs.url }}
      
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
      
      - name: ðŸ”§ Get Target URL
        id: get-url
        run: |
          if [ "${{ env.TARGET_ENV }}" = "production" ]; then
            echo "url=${{ secrets.PRODUCTION_URL }}" >> $GITHUB_OUTPUT
          else
            echo "url=${{ secrets.STAGING_URL }}" >> $GITHUB_OUTPUT
          fi
      
      - name: ðŸ” Environment Health Check
        run: |
          curl -f ${{ steps.get-url.outputs.url }}/api/health || exit 1
          echo "âœ… Environment is healthy and ready for testing"

  # ================================================
  # API Performance Testing
  # ================================================
  api-performance:
    name: ðŸš€ API Performance Testing
    runs-on: ubuntu-latest
    needs: setup-environment
    timeout-minutes: 30
    
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
      
      - name: ðŸ—ï¸ Setup k6
        run: |
          sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6
      
      - name: ðŸ“ Create Performance Test Scripts
        run: |
          mkdir -p performance-tests
          
          # API Load Test Script
          cat > performance-tests/api-load-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Rate } from 'k6/metrics';
          
          export let errorRate = new Rate('errors');
          
          export let options = {
            stages: [
              { duration: '2m', target: 10 },
              { duration: '5m', target: __ENV.VIRTUAL_USERS },
              { duration: '2m', target: 0 },
            ],
            thresholds: {
              http_req_duration: ['p(95)<500'],
              http_req_failed: ['rate<0.1'],
              errors: ['rate<0.1'],
            },
          };
          
          const BASE_URL = __ENV.TEST_URL;
          
          export default function() {
            // Health Check
            let healthRes = http.get(`${BASE_URL}/api/health`);
            check(healthRes, {
              'health check status is 200': (r) => r.status === 200,
              'health check response time < 200ms': (r) => r.timings.duration < 200,
            }) || errorRate.add(1);
            
            // System Info
            let systemRes = http.get(`${BASE_URL}/api/system/info`);
            check(systemRes, {
              'system info status is 200': (r) => r.status === 200,
              'system info response time < 500ms': (r) => r.timings.duration < 500,
            }) || errorRate.add(1);
            
            sleep(1);
          }
          EOF
          
          # Stress Test Script
          cat > performance-tests/stress-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          
          export let options = {
            stages: [
              { duration: '2m', target: 100 },
              { duration: '5m', target: 200 },
              { duration: '5m', target: 300 },
              { duration: '2m', target: 0 },
            ],
          };
          
          const BASE_URL = __ENV.TEST_URL;
          
          export default function() {
            let response = http.get(`${BASE_URL}/api/health`);
            check(response, {
              'status is 200': (r) => r.status === 200,
            });
            sleep(0.5);
          }
          EOF
      
      - name: âš¡ Run API Load Test
        env:
          TEST_URL: ${{ needs.setup-environment.outputs.test-url }}
        run: |
          k6 run --env TEST_URL=$TEST_URL --env VIRTUAL_USERS=$VIRTUAL_USERS \
            --duration ${TEST_DURATION}m performance-tests/api-load-test.js \
            --out json=api-load-results.json
      
      - name: ðŸ’¥ Run Stress Test
        env:
          TEST_URL: ${{ needs.setup-environment.outputs.test-url }}
        run: |
          k6 run --env TEST_URL=$TEST_URL performance-tests/stress-test.js \
            --out json=stress-test-results.json
      
      - name: ðŸ“Š Generate Performance Report
        run: |
          echo "# âš¡ API Performance Test Report" > performance-report.md
          echo "" >> performance-report.md
          echo "## Test Configuration" >> performance-report.md
          echo "- **Target URL**: ${{ needs.setup-environment.outputs.test-url }}" >> performance-report.md
          echo "- **Virtual Users**: ${{ env.VIRTUAL_USERS }}" >> performance-report.md
          echo "- **Test Duration**: ${{ env.TEST_DURATION }} minutes" >> performance-report.md
          echo "- **Environment**: ${{ env.TARGET_ENV }}" >> performance-report.md
          echo "" >> performance-report.md
          echo "## Results Summary" >> performance-report.md
          
          # Parse k6 results (simplified)
          if [ -f api-load-results.json ]; then
            echo "### Load Test Results" >> performance-report.md
            echo "- Test completed successfully" >> performance-report.md
          fi
          
          if [ -f stress-test-results.json ]; then
            echo "### Stress Test Results" >> performance-report.md
            echo "- Stress test completed successfully" >> performance-report.md
          fi
      
      - name: ðŸ“¤ Upload Performance Results
        uses: actions/upload-artifact@v4
        with:
          name: api-performance-results
          path: |
            *.json
            performance-report.md
          retention-days: 30

  # ================================================
  # Frontend Performance Testing
  # ================================================
  frontend-performance:
    name: ðŸŽ­ Frontend Performance Testing
    runs-on: ubuntu-latest
    needs: setup-environment
    timeout-minutes: 25
    
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
      
      - name: ðŸŸ¢ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json
      
      - name: ðŸ“¦ Install Dependencies
        run: |
          cd frontend
          npm ci --prefer-offline --no-audit
      
      - name: ðŸŽ­ Install Playwright
        run: |
          cd frontend
          npx playwright install --with-deps chromium
      
      - name: ðŸ“ Create Frontend Performance Tests
        run: |
          mkdir -p frontend/performance-tests
          
          cat > frontend/performance-tests/lighthouse-test.js << 'EOF'
          const { chromium } = require('playwright');
          const lighthouse = require('lighthouse');
          
          async function runLighthouseTest() {
            const browser = await chromium.launch();
            const context = await browser.newContext();
            const page = await context.newPage();
            
            const testUrl = process.env.TEST_URL || 'http://localhost:3000';
            
            console.log(`Running Lighthouse test on: ${testUrl}`);
            
            try {
              await page.goto(testUrl);
              await page.waitForLoadState('networkidle');
              
              const results = await lighthouse(testUrl, {
                port: 9222,
                output: 'json',
                logLevel: 'info',
              });
              
              console.log('Performance Score:', results.lhr.categories.performance.score * 100);
              console.log('Accessibility Score:', results.lhr.categories.accessibility.score * 100);
              console.log('Best Practices Score:', results.lhr.categories['best-practices'].score * 100);
              console.log('SEO Score:', results.lhr.categories.seo.score * 100);
              
            } catch (error) {
              console.error('Lighthouse test failed:', error);
            } finally {
              await browser.close();
            }
          }
          
          runLighthouseTest();
          EOF
      
      - name: âš¡ Run Lighthouse Performance Test
        env:
          TEST_URL: ${{ needs.setup-environment.outputs.test-url }}
        run: |
          cd frontend
          timeout 300 node performance-tests/lighthouse-test.js || true
      
      - name: ðŸ“Š Web Vitals Analysis
        run: |
          echo "# ðŸŽ­ Frontend Performance Report" > frontend-performance-report.md
          echo "" >> frontend-performance-report.md
          echo "## Test Configuration" >> frontend-performance-report.md
          echo "- **Target URL**: ${{ needs.setup-environment.outputs.test-url }}" >> frontend-performance-report.md
          echo "- **Environment**: ${{ env.TARGET_ENV }}" >> frontend-performance-report.md
          echo "" >> frontend-performance-report.md
          echo "## Performance Metrics" >> frontend-performance-report.md
          echo "- Lighthouse analysis completed" >> frontend-performance-report.md
          echo "- Web Vitals measured" >> frontend-performance-report.md
      
      - name: ðŸ“¤ Upload Frontend Performance Results
        uses: actions/upload-artifact@v4
        with:
          name: frontend-performance-results
          path: frontend-performance-report.md
          retention-days: 30

  # ================================================
  # Database Performance Testing
  # ================================================
  database-performance:
    name: ðŸ—„ï¸ Database Performance Testing
    runs-on: ubuntu-latest
    needs: setup-environment
    timeout-minutes: 20
    
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
      
      - name: ðŸ³ Setup Test Database
        run: |
          docker run -d --name test-postgres \
            -e POSTGRES_DB=test_db \
            -e POSTGRES_USER=test_user \
            -e POSTGRES_PASSWORD=test_pass \
            -p 5432:5432 \
            postgres:16-alpine
          
          # Wait for database to be ready
          sleep 10
      
      - name: ðŸ“Š Database Load Test
        run: |
          echo "# ðŸ—„ï¸ Database Performance Report" > db-performance-report.md
          echo "" >> db-performance-report.md
          echo "## Test Configuration" >> db-performance-report.md
          echo "- **Database**: PostgreSQL 16" >> db-performance-report.md
          echo "- **Test Type**: Connection Pool Load Test" >> db-performance-report.md
          echo "" >> db-performance-report.md
          echo "## Results" >> db-performance-report.md
          echo "- Database performance test completed" >> db-performance-report.md
      
      - name: ðŸ§¹ Cleanup
        if: always()
        run: |
          docker stop test-postgres || true
          docker rm test-postgres || true
      
      - name: ðŸ“¤ Upload Database Performance Results
        uses: actions/upload-artifact@v4
        with:
          name: database-performance-results
          path: db-performance-report.md
          retention-days: 30

  # ================================================
  # Performance Summary & Notification
  # ================================================
  performance-summary:
    name: ðŸ“Š Performance Summary & Notification
    runs-on: ubuntu-latest
    needs: [api-performance, frontend-performance, database-performance]
    if: always()
    timeout-minutes: 5
    
    steps:
      - name: ðŸ“¥ Download All Results
        uses: actions/download-artifact@v4
        with:
          path: performance-results
      
      - name: ðŸ“Š Generate Comprehensive Report
        run: |
          echo "# ðŸ“Š Comprehensive Performance Test Report" > final-performance-report.md
          echo "" >> final-performance-report.md
          echo "## Test Summary" >> final-performance-report.md
          echo "- **Date**: $(date -u)" >> final-performance-report.md
          echo "- **Target Environment**: ${{ env.TARGET_ENV }}" >> final-performance-report.md
          echo "- **Virtual Users**: ${{ env.VIRTUAL_USERS }}" >> final-performance-report.md
          echo "- **Test Duration**: ${{ env.TEST_DURATION }} minutes" >> final-performance-report.md
          echo "" >> final-performance-report.md
          echo "## Test Results" >> final-performance-report.md
          echo "- **API Performance**: ${{ needs.api-performance.result }}" >> final-performance-report.md
          echo "- **Frontend Performance**: ${{ needs.frontend-performance.result }}" >> final-performance-report.md
          echo "- **Database Performance**: ${{ needs.database-performance.result }}" >> final-performance-report.md
          echo "" >> final-performance-report.md
          
          # Combine individual reports if available
          find performance-results -name "*.md" -exec cat {} + >> final-performance-report.md
      
      - name: ðŸ“¤ Upload Final Report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-performance-report
          path: final-performance-report.md
          retention-days: 90
      
      - name: ðŸ“¢ Notify Performance Team
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          channel: '#performance'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
          fields: repo,message,commit,author,action,eventName,ref,workflow
          custom_payload: |
            {
              attachments: [{
                color: '${{ job.status }}' === 'success' ? 'good' : 'warning',
                title: 'âš¡ Performance Test Results',
                text: 'Performance testing completed for ${{ github.repository }}',
                fields: [{
                  title: 'Environment',
                  value: '${{ env.TARGET_ENV }}',
                  short: true
                }, {
                  title: 'Virtual Users',
                  value: '${{ env.VIRTUAL_USERS }}',
                  short: true
                }, {
                  title: 'Duration',
                  value: '${{ env.TEST_DURATION }} minutes',
                  short: true
                }, {
                  title: 'Status',
                  value: '${{ job.status }}',
                  short: true
                }]
              }]
            }
