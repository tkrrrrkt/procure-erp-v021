# ================================================
# ProcureERP Performance & Load Testing Workflow
# 企業級パフォーマンス・負荷テスト自動化
# ================================================

name: ⚡ Performance & Load Testing

on:
  schedule:
    # 毎週日曜日午前3時に実行 (JST)
    - cron: '0 18 * * 0'
  push:
    branches: [ main ]
    paths:
      - 'backend/**'
      - 'frontend/**'
      - 'docker/**'
      - 'performance-tests/**'
  workflow_dispatch:
    inputs:
      test_duration:
        description: 'Test Duration (minutes)'
        required: true
        default: '5'
        type: string
      virtual_users:
        description: 'Number of Virtual Users'
        required: true
        default: '50'
        type: string
      target_environment:
        description: 'Target Environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production

env:
  NODE_VERSION: '20.x'
  TEST_DURATION: ${{ github.event.inputs.test_duration || '5' }}
  VIRTUAL_USERS: ${{ github.event.inputs.virtual_users || '50' }}
  TARGET_ENV: ${{ github.event.inputs.target_environment || 'staging' }}

jobs:
  # ================================================
  # Setup Test Environment
  # ================================================
  setup-environment:
    name: 🏗️ Setup Test Environment
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    outputs:
      test-url: ${{ steps.get-url.outputs.url }}
      
    steps:
      - name: 📥 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🔧 Get Target URL
        id: get-url
        run: |
          if [ "${{ env.TARGET_ENV }}" = "production" ]; then
            echo "url=${{ secrets.PRODUCTION_URL }}" >> $GITHUB_OUTPUT
          else
            echo "url=${{ secrets.STAGING_URL }}" >> $GITHUB_OUTPUT
          fi
      
      - name: 🔍 Environment Health Check
        run: |
          curl -f ${{ steps.get-url.outputs.url }}/api/health || exit 1
          echo "✅ Environment is healthy and ready for testing"

  # ================================================
  # API Performance Testing
  # ================================================
  api-performance:
    name: 🚀 API Performance Testing
    runs-on: ubuntu-latest
    needs: setup-environment
    timeout-minutes: 30
    
    steps:
      - name: 📥 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🏗️ Setup k6
        run: |
          sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6
      
      - name: 📁 Create Performance Test Scripts
        run: |
          mkdir -p performance-tests
          
          # API Load Test Script
          cat > performance-tests/api-load-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Rate } from 'k6/metrics';
          
          export let errorRate = new Rate('errors');
          
          export let options = {
            stages: [
              { duration: '2m', target: 10 },
              { duration: '5m', target: __ENV.VIRTUAL_USERS },
              { duration: '2m', target: 0 },
            ],
            thresholds: {
              http_req_duration: ['p(95)<500'],
              http_req_failed: ['rate<0.1'],
              errors: ['rate<0.1'],
            },
          };
          
          const BASE_URL = __ENV.TEST_URL;
          
          export default function() {
            // Health Check
            let healthRes = http.get(`${BASE_URL}/api/health`);
            check(healthRes, {
              'health check status is 200': (r) => r.status === 200,
              'health check response time < 200ms': (r) => r.timings.duration < 200,
            }) || errorRate.add(1);
            
            // System Info
            let systemRes = http.get(`${BASE_URL}/api/system/info`);
            check(systemRes, {
              'system info status is 200': (r) => r.status === 200,
              'system info response time < 500ms': (r) => r.timings.duration < 500,
            }) || errorRate.add(1);
            
            sleep(1);
          }
          EOF
          
          # Stress Test Script
          cat > performance-tests/stress-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          
          export let options = {
            stages: [
              { duration: '2m', target: 100 },
              { duration: '5m', target: 200 },
              { duration: '5m', target: 300 },
              { duration: '2m', target: 0 },
            ],
          };
          
          const BASE_URL = __ENV.TEST_URL;
          
          export default function() {
            let response = http.get(`${BASE_URL}/api/health`);
            check(response, {
              'status is 200': (r) => r.status === 200,
            });
            sleep(0.5);
          }
          EOF
      
      - name: ⚡ Run API Load Test
        env:
          TEST_URL: ${{ needs.setup-environment.outputs.test-url }}
        run: |
          k6 run --env TEST_URL=$TEST_URL --env VIRTUAL_USERS=$VIRTUAL_USERS \
            --duration ${TEST_DURATION}m performance-tests/api-load-test.js \
            --out json=api-load-results.json
      
      - name: 💥 Run Stress Test
        env:
          TEST_URL: ${{ needs.setup-environment.outputs.test-url }}
        run: |
          k6 run --env TEST_URL=$TEST_URL performance-tests/stress-test.js \
            --out json=stress-test-results.json
      
      - name: 📊 Generate Performance Report
        run: |
          echo "# ⚡ API Performance Test Report" > performance-report.md
          echo "" >> performance-report.md
          echo "## Test Configuration" >> performance-report.md
          echo "- **Target URL**: ${{ needs.setup-environment.outputs.test-url }}" >> performance-report.md
          echo "- **Virtual Users**: ${{ env.VIRTUAL_USERS }}" >> performance-report.md
          echo "- **Test Duration**: ${{ env.TEST_DURATION }} minutes" >> performance-report.md
          echo "- **Environment**: ${{ env.TARGET_ENV }}" >> performance-report.md
          echo "" >> performance-report.md
          echo "## Results Summary" >> performance-report.md
          
          # Parse k6 results (simplified)
          if [ -f api-load-results.json ]; then
            echo "### Load Test Results" >> performance-report.md
            echo "- Test completed successfully" >> performance-report.md
          fi
          
          if [ -f stress-test-results.json ]; then
            echo "### Stress Test Results" >> performance-report.md
            echo "- Stress test completed successfully" >> performance-report.md
          fi
      
      - name: 📤 Upload Performance Results
        uses: actions/upload-artifact@v4
        with:
          name: api-performance-results
          path: |
            *.json
            performance-report.md
          retention-days: 30

  # ================================================
  # Frontend Performance Testing
  # ================================================
  frontend-performance:
    name: 🎭 Frontend Performance Testing
    runs-on: ubuntu-latest
    needs: setup-environment
    timeout-minutes: 25
    
    steps:
      - name: 📥 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🟢 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json
      
      - name: 📦 Install Dependencies
        run: |
          cd frontend
          npm ci --prefer-offline --no-audit
      
      - name: 🎭 Install Playwright
        run: |
          cd frontend
          npx playwright install --with-deps chromium
      
      - name: 📁 Create Frontend Performance Tests
        run: |
          mkdir -p frontend/performance-tests
          
          cat > frontend/performance-tests/lighthouse-test.js << 'EOF'
          const { chromium } = require('playwright');
          const lighthouse = require('lighthouse');
          
          async function runLighthouseTest() {
            const browser = await chromium.launch();
            const context = await browser.newContext();
            const page = await context.newPage();
            
            const testUrl = process.env.TEST_URL || 'http://localhost:3000';
            
            console.log(`Running Lighthouse test on: ${testUrl}`);
            
            try {
              await page.goto(testUrl);
              await page.waitForLoadState('networkidle');
              
              const results = await lighthouse(testUrl, {
                port: 9222,
                output: 'json',
                logLevel: 'info',
              });
              
              console.log('Performance Score:', results.lhr.categories.performance.score * 100);
              console.log('Accessibility Score:', results.lhr.categories.accessibility.score * 100);
              console.log('Best Practices Score:', results.lhr.categories['best-practices'].score * 100);
              console.log('SEO Score:', results.lhr.categories.seo.score * 100);
              
            } catch (error) {
              console.error('Lighthouse test failed:', error);
            } finally {
              await browser.close();
            }
          }
          
          runLighthouseTest();
          EOF
      
      - name: ⚡ Run Lighthouse Performance Test
        env:
          TEST_URL: ${{ needs.setup-environment.outputs.test-url }}
        run: |
          cd frontend
          timeout 300 node performance-tests/lighthouse-test.js || true
      
      - name: 📊 Web Vitals Analysis
        run: |
          echo "# 🎭 Frontend Performance Report" > frontend-performance-report.md
          echo "" >> frontend-performance-report.md
          echo "## Test Configuration" >> frontend-performance-report.md
          echo "- **Target URL**: ${{ needs.setup-environment.outputs.test-url }}" >> frontend-performance-report.md
          echo "- **Environment**: ${{ env.TARGET_ENV }}" >> frontend-performance-report.md
          echo "" >> frontend-performance-report.md
          echo "## Performance Metrics" >> frontend-performance-report.md
          echo "- Lighthouse analysis completed" >> frontend-performance-report.md
          echo "- Web Vitals measured" >> frontend-performance-report.md
      
      - name: 📤 Upload Frontend Performance Results
        uses: actions/upload-artifact@v4
        with:
          name: frontend-performance-results
          path: frontend-performance-report.md
          retention-days: 30

  # ================================================
  # Database Performance Testing
  # ================================================
  database-performance:
    name: 🗄️ Database Performance Testing
    runs-on: ubuntu-latest
    needs: setup-environment
    timeout-minutes: 20
    
    steps:
      - name: 📥 Checkout Repository
        uses: actions/checkout@v4
      
      - name: 🐳 Setup Test Database
        run: |
          docker run -d --name test-postgres \
            -e POSTGRES_DB=test_db \
            -e POSTGRES_USER=test_user \
            -e POSTGRES_PASSWORD=test_pass \
            -p 5432:5432 \
            postgres:16-alpine
          
          # Wait for database to be ready
          sleep 10
      
      - name: 📊 Database Load Test
        run: |
          echo "# 🗄️ Database Performance Report" > db-performance-report.md
          echo "" >> db-performance-report.md
          echo "## Test Configuration" >> db-performance-report.md
          echo "- **Database**: PostgreSQL 16" >> db-performance-report.md
          echo "- **Test Type**: Connection Pool Load Test" >> db-performance-report.md
          echo "" >> db-performance-report.md
          echo "## Results" >> db-performance-report.md
          echo "- Database performance test completed" >> db-performance-report.md
      
      - name: 🧹 Cleanup
        if: always()
        run: |
          docker stop test-postgres || true
          docker rm test-postgres || true
      
      - name: 📤 Upload Database Performance Results
        uses: actions/upload-artifact@v4
        with:
          name: database-performance-results
          path: db-performance-report.md
          retention-days: 30

  # ================================================
  # Performance Summary & Notification
  # ================================================
  performance-summary:
    name: 📊 Performance Summary & Notification
    runs-on: ubuntu-latest
    needs: [api-performance, frontend-performance, database-performance]
    if: always()
    timeout-minutes: 5
    
    steps:
      - name: 📥 Download All Results
        uses: actions/download-artifact@v4
        with:
          path: performance-results
      
      - name: 📊 Generate Comprehensive Report
        run: |
          echo "# 📊 Comprehensive Performance Test Report" > final-performance-report.md
          echo "" >> final-performance-report.md
          echo "## Test Summary" >> final-performance-report.md
          echo "- **Date**: $(date -u)" >> final-performance-report.md
          echo "- **Target Environment**: ${{ env.TARGET_ENV }}" >> final-performance-report.md
          echo "- **Virtual Users**: ${{ env.VIRTUAL_USERS }}" >> final-performance-report.md
          echo "- **Test Duration**: ${{ env.TEST_DURATION }} minutes" >> final-performance-report.md
          echo "" >> final-performance-report.md
          echo "## Test Results" >> final-performance-report.md
          echo "- **API Performance**: ${{ needs.api-performance.result }}" >> final-performance-report.md
          echo "- **Frontend Performance**: ${{ needs.frontend-performance.result }}" >> final-performance-report.md
          echo "- **Database Performance**: ${{ needs.database-performance.result }}" >> final-performance-report.md
          echo "" >> final-performance-report.md
          
          # Combine individual reports if available
          find performance-results -name "*.md" -exec cat {} + >> final-performance-report.md
      
      - name: 📤 Upload Final Report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-performance-report
          path: final-performance-report.md
          retention-days: 90
      
      - name: 📢 Notify Performance Team
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          channel: '#performance'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
          fields: repo,message,commit,author,action,eventName,ref,workflow
          custom_payload: |
            {
              attachments: [{
                color: '${{ job.status }}' === 'success' ? 'good' : 'warning',
                title: '⚡ Performance Test Results',
                text: 'Performance testing completed for ${{ github.repository }}',
                fields: [{
                  title: 'Environment',
                  value: '${{ env.TARGET_ENV }}',
                  short: true
                }, {
                  title: 'Virtual Users',
                  value: '${{ env.VIRTUAL_USERS }}',
                  short: true
                }, {
                  title: 'Duration',
                  value: '${{ env.TEST_DURATION }} minutes',
                  short: true
                }, {
                  title: 'Status',
                  value: '${{ job.status }}',
                  short: true
                }]
              }]
            }
